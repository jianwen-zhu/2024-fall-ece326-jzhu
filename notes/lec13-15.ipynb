{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47270a59-3288-49b0-a432-a234b069fa6b",
   "metadata": {},
   "source": [
    "# Array Programming Paradigm (Einstein's Notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357a4e8-18ad-4edd-a073-376d79cbd32f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "* We have seen Iverson's Notation\n",
    "* And its implementation in Numpy\n",
    "* Take Away:\n",
    "  - Every value is an array\n",
    "  - Operator as function of functions\n",
    "  - Map-Reduce pattern: Matrix algorithms / Graph algorithms\n",
    "* Generalize to N-Dimensions: Tensor functions\n",
    "  - Growing bags of API and functions\n",
    "    - Lots of mental burdens\n",
    "    - Not portable among different implementation frameworks\n",
    "    - Error prone\n",
    "  - Is there a notation that rules them all?\n",
    "  - Of course we have Einstein who likes to have theory of everything\n",
    "  - Einstein Notation: Not invented, by popularized by Einstein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce101df6-5609-49e2-92f3-884796cad956",
   "metadata": {},
   "source": [
    "## Basic Einsum Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6a5013-9259-4e0c-9cf3-636fdee1c7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.arange(9).reshape( 3, 3 )\n",
    "B = np.arange(9).reshape( 3, 3 )\n",
    "print( A )\n",
    "print( B )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ece579-e9ff-49ce-aedb-90e497b03447",
   "metadata": {},
   "source": [
    "Let's Look at Matric Multiplication again. Below is a typical imperative code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20476e80-3c8b-4832-9d30-ef4388986dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = A.shape[0]\n",
    "C = np.zeros( (N,N), dtype=int )\n",
    "for i in range(N) :\n",
    "    for j in range(N) :\n",
    "        for k in range( N ) :\n",
    "            C[i,j] += A[i,k] * B[k,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2548cf7-4cf3-4cc7-9ec3-fa819f9a62b9",
   "metadata": {},
   "source": [
    "This works, but Einstein stared at it and figure out that the loop nest is really not necessary and can be made *implicit*. Also if we know we keep doing map-reduce on a specif semi-ring (don't worry about the math jagon, but just say it is map-reduce on *, +), we don't need them explicit either. This just leaves us the subscript! \n",
    "\n",
    "So we cut the verbosity and it becomes as terse as the following: \n",
    "\n",
    "ik, kj -> ij     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e1b18-95e5-4173-be39-746aa6ea573b",
   "metadata": {},
   "source": [
    "Someone (We actually should give credit to Mark Wiebe, the author) actually implemented the notation in Numpy and call it the einsum (Einstein Sum) function. Einsum looks like the following:\n",
    "\n",
    "~~~\n",
    "result = np.einsum( format, arg1, arg2, ..., argn )\n",
    "~~~\n",
    "\n",
    "format is a string using the Einsein notation, and gives the complete spec of a function over arg1, arg2, ... argn, each a N-D (tensor) value; and returns another tensor value. \n",
    "\n",
    "Of course the format string has to match the dimensions:\n",
    "\n",
    "~~~\n",
    "s = np.einsum( \"a->\", v )\n",
    "T = np.einsum( \"ij->ji\", M )\n",
    "C = np.einsum( \"mn,np->mp\", A, B )\n",
    "\n",
    "assert v.ndim == len( \"a\" )\n",
    "assert s.ndim == len( \"\" )\n",
    "\n",
    "assert M.ndim == len( \"ij\" )\n",
    "assert T.ndim == len( \"ji\" )\n",
    "\n",
    "assert A.ndim == len( \"mn\" )\n",
    "assert B.ndim == len( \"np\" )\n",
    "assert C.ndim == len( \"mp\" )\n",
    "~~~\n",
    "\n",
    "Comparing to its nested loop counterpart, the einsum function is super compact!\n",
    "\n",
    "Let's first see a few examples to get a flavour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f1da2-7172-4f03-b18e-7ecb6ccafd4f",
   "metadata": {},
   "source": [
    "### 2-D Matrix Diagonal Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75e52e5d-a679-41d6-b283-6eeb54450509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[0 4 8]\n",
      "[0 4 8]\n"
     ]
    }
   ],
   "source": [
    "d = np.zeros( (N), dtype=int )\n",
    "for i in range( N ) :\n",
    "    d[i] += A[i,i]\n",
    "\n",
    "d2 = np.einsum( \"ii->i\", A )\n",
    "print( A )\n",
    "print( d )\n",
    "print( d2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c5a9b-12fb-4da2-92fc-ec230b1a311e",
   "metadata": {},
   "source": [
    "### 2-D Matrix Trace\n",
    "\n",
    "Recall the term \"trace\" in Linear Algebra? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6295af54-7664-4c8a-8ecb-b38a340e2fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "d = 0\n",
    "for i in range( N ) :\n",
    "    d += A[i,i]\n",
    "d2 = np.einsum( \"ii->\", A )\n",
    "\n",
    "print( A )\n",
    "print( d )\n",
    "print( d2 )\n",
    "assert np.allclose( d, d2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb581f-860a-48cd-ba41-75b28d3d6ae2",
   "metadata": {},
   "source": [
    "### Three Argument Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d00d08a-35d1-431f-a57f-ff530a4b2308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "v = np.arange( N )\n",
    "d = 0\n",
    "for s in range( N ) :\n",
    "    for t in range( N ) :\n",
    "        d += v[s] * A[s,t] * v[t]\n",
    "d2 = np.einsum( \"s,st,t->\", v, A, v )\n",
    "\n",
    "print( v )\n",
    "print( d )\n",
    "print( d2 )\n",
    "assert np.allclose( d, d2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d5324-0868-413c-80ab-e37619cdda7f",
   "metadata": {},
   "source": [
    "### Batched Outer Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40064046-ac54-4df1-81e4-a87ce472dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros( (N,N,N), dtype=int )\n",
    "for b in range( N ) :\n",
    "    for i in range( N ) :\n",
    "        for j in range( N ) :\n",
    "            d[b,i,j] += A[b,i] * B[b,j]\n",
    "d2 = np.einsum( \"bi,bj->bij\", A, B )\n",
    "\n",
    "assert np.allclose( d, d2 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c4ccf-ce7a-40b8-bb5b-8ea826095201",
   "metadata": {},
   "source": [
    "### Semantics\n",
    "\n",
    "In the above examples, we didn't care how functions are named and called, we just use a *canonical notation* and gets the work done. So how exactly an einsum is evaluated, in other words, what is its semantics (meaning)? \n",
    "\n",
    "We distingush two type of indices:\n",
    "\n",
    "1. Free Indices:\n",
    "\n",
    "   Indices *present* in the output indices;\n",
    "   \n",
    "1. Summation (Reduction) Indices: \n",
    "\n",
    "    Indices *disappeared* in the output indices.\n",
    "\n",
    "Typically, free indices are in the outer loop, summation indices are in the inner loop.\n",
    "As you recall, summation (reduction) carries out *dimension reduction*: that dimension will be GONE at the output. \n",
    "\n",
    "Now let's go back to the previous examples again and identify the free and summation indices. \n",
    "\n",
    "Also note that Einsum does NOT impose any evaluation order. In fact, the order of the indices in the format string does not matter at all, only their precense, and whether they the index label is repeated matters (that means axis of different inputs are aligned)! \n",
    "\n",
    "That said, the different evaluation order matters a lot in the implementation (recall the parallel map-reduce example?), but that's somebody else job, and at this stage, we make sure we have the right abstraction (so we don't OVER specify), and give the implementation frameworks enough freedom to do their job well.\n",
    "\n",
    "There is a special case when we leave out the arrow ('->'). In this case, Numpy einsum will take the labels that appeared once and arrange them in alphabetical order. So 'ij, jk' is equivalent to 'ij, ji -> ik'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2f4018-33d4-4df9-b77d-878bf1b2ebe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 6]\n",
      " [1 4 7]\n",
      " [2 5 8]]\n",
      "[[0 3 6]\n",
      " [1 4 7]\n",
      " [2 5 8]]\n"
     ]
    }
   ],
   "source": [
    "print( np.einsum( 'ji', A ) )\n",
    "print( np.einsum( 'ji->ij', A ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2116976-2919-4d31-a39d-9330de18cf0a",
   "metadata": {},
   "source": [
    "### Wrong Uses\n",
    "\n",
    "~~~\n",
    "#    Bad. Number of input index groups doesn't match number of\n",
    "#         arguments.\n",
    "np.einsum(\"ab,bc->ac\", A)\n",
    " \n",
    "#    Bad. Indexes must be ASCII upper/lowercase letters.\n",
    "np.einsum(\"012,1^%->:;?\", A, B)\n",
    " \n",
    "#    Bad. Argument 0 has 3 dimensions but only 2 indexes are\n",
    "#         given.\n",
    "A = np.random.normal(size = (2,3,4))\n",
    "B = np.random.normal(size = (4,5,6))\n",
    "np.einsum(\"ab,bcd->a\", A, B)\n",
    " \n",
    "#    Bad. One of the output indexes isn't in the set of all\n",
    "#         input indexes.\n",
    "np.einsum(\"ab,bc->acz\", A, B)\n",
    " \n",
    "#    Bad. Output has a repeated index.\n",
    "np.einsum(\"ab,bc->baa\", A, B)\n",
    "\n",
    "\n",
    "#    Bad. Mismatches in the sizes of input argument axes\n",
    "#         that are labelled with the same index.\n",
    "A = np.random.normal(size = (2,3,4))\n",
    "B = np.random.normal(size = (3,4,5))\n",
    "np.einsum(\"ckj,cqq->c\", A, B)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5501a44-b7f2-4a8c-a834-e46ac2670ff3",
   "metadata": {},
   "source": [
    "### Canonical Notation\n",
    "\n",
    "By Canonical we mean there is just ONE way of express something. \n",
    "\n",
    "Let's first look at 1D Vector functions.\n",
    "\n",
    "| Einsum             | Numpy Equivalent  | Description         |\n",
    "| ------------------ | ----------------- | ---                 |\n",
    "| ('i', A)           | A                 | return a view of A  |\n",
    "| ('i->', A)         | sum(A)            | sum all elements of A |\n",
    "| ('i,i->i, A, B)    | A*B               | element-wise mult of two vectors |\n",
    "| ('i,i->', A, B)    | inner(A, B)       | inner product of two vectors |\n",
    "| ('i,j->ij', A, B)  | outer(A,B)        | outer product of two vectors |\n",
    "\n",
    "Now let's look at 2D Matrix functions over matrices with compatible shapes.\n",
    "\n",
    "| Einsum             | Numpy Equivalent  | Description         |\n",
    "| ------------------ | ----------------- | ---                 |\n",
    "| ('ij', A)          | A                 | a View of itself    |\n",
    "| ('ji', A)          | A.T               | Transpose | \n",
    "| ('ii->i', A)       | diag(A)           | main diagnoal of a matrix |\n",
    "| ('ij->', A)        | sum(A)            | reduction on ALL elements |\n",
    "| ('ij->j', A)       | sum(A,axis=0)     | reduction on ALL rows |\n",
    "| ('ij->i', A)       | sum(A,axis=1)     | reduction on ALL columns | \n",
    "| ('ij,ij->ij', A, B) |\tA * B\t         | Hadamard product: element-wise multiplication of A and B |\n",
    "| ('ij,ji->ij', A, B) |\tA * B.T\t         | element-wise multiplication of A and B.T |\n",
    "| ('ij,jk', A, B)\t  | dot(A, B)\t     | matrix multiplication of A and B |\n",
    "| ('ij,kj->ik', A, B) | inner(A, B)\t     | inner product of A and B |\n",
    "| ('ij,kj->ikj', A, B) | A[:, None] * B\t  | each row of A multiplied by B |\n",
    "| ('ij,kl->ijkl', A, B)\t| A[:, :, None, None] * B\t| each value of A multiplied by B|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c582b-7b78-4df0-9a0a-cf2359ec1fbd",
   "metadata": {},
   "source": [
    "### Marching to Higher Dimensions\n",
    "\n",
    "Einsum really shines when dealing with tensors beyond 2 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f14b7d-06a6-4a42-9c5c-3c8da9432060",
   "metadata": {},
   "source": [
    "* Batch Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848880a8-c184-4306-93de-ddd88836a5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  70   76   82   88   94]\n",
      "  [ 190  212  234  256  278]\n",
      "  [ 310  348  386  424  462]]\n",
      "\n",
      " [[1510 1564 1618 1672 1726]\n",
      "  [1950 2020 2090 2160 2230]\n",
      "  [2390 2476 2562 2648 2734]]]\n"
     ]
    }
   ],
   "source": [
    "# Batch matrix multiplication\n",
    "X = np.arange(24).reshape(2, 3, 4)\n",
    "Y = np.arange(40).reshape(2, 4, 5)\n",
    "\n",
    "A = np.einsum('ijk, ikl->ijl', X, Y)\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd495c6-167c-4e03-a4c0-31efd1ebffa7",
   "metadata": {},
   "source": [
    "In this example, i,j,l are free indices, and k is the summation index. We extend our familiar matrix multiplication like behavior to three dimensional arrays. It can be thought of performing matrix multiplications on batches of data, hence the name. But in Einstin notation, there is really not much difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad61b58-0f41-4569-bac4-a5891d7da31c",
   "metadata": {},
   "source": [
    "* Tensor Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be85e3fc-6e15-46cb-869d-c71e646e3ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 11, 13, 17)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand( 2,3,5,7 )\n",
    "Y = np.random.rand( 11,13,3,17,5 )\n",
    "A = np.einsum('pqrs,tuqvr->pstuv', X, Y)\n",
    "print( A.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce28036-e974-49e7-a0f4-54d4b0bb5ed1",
   "metadata": {},
   "source": [
    "## Battery: Einops, or Generalized Einsum\n",
    "\n",
    "While Numpy einsum function is pioneering and extremely useful, it has a few limitations: \n",
    "\n",
    "1. It does not support reduction on arbitary functions. Only on addition.\n",
    "2. Only allows on character to name axis, and gets error-prone for complex problems;\n",
    "3. Implicit indexing relies on character sorting, and contradict the spirit of making explicit declarations.\n",
    "\n",
    "Inspired on Numpy Einsum function, Alex Rogozhnikov implemented the Einops package, which revised a few design decisions regarding the Einsum Function itself. \n",
    "\n",
    "1. Use full identifier as name of axis, and use space as deliminator;\n",
    "2. Format string is at the end (not at the beginning);\n",
    "3. Implicit indexing is forbidden.\n",
    "\n",
    "You could download the package yourself. \n",
    "\n",
    "~~~\n",
    "%git clone https://github.com/arogozhnikov/einops.git\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9adacb3-e75c-4fb0-b752-ffd98bcf016e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 12, 21])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import einsum\n",
    "\n",
    "einsum( A, 'row col -> row' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b52399-bee0-4422-8998-43c6d67c5a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15,  18,  21],\n",
       "       [ 42,  54,  66],\n",
       "       [ 69,  90, 111]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einsum( A, B, 'i k, k j -> i j' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74018bd7-c8d3-4103-9fd5-f767bc497a64",
   "metadata": {},
   "source": [
    "It also added explicit reduction function on arbitary functions and also a careful thoughtout, but very small set of tensor \"rearranging\" construct.\n",
    "\n",
    "Next We will now go directly to the tutorial from the package documentation itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082508b0-952b-4c01-9234-8e331e5c78fe",
   "metadata": {},
   "source": [
    "## Take Away\n",
    "\n",
    "Einstein's notation, in the same spirit of Iverson's APL notation, encourages abstract thinking, and reduces a plethora of seemingly different things into a handful of concepts.\n",
    "\n",
    "With engineering efforts progressed through Numpy and Einops (2018), the abstract thinking can translate to real world impact. For example, Einops not only admits extremely terse expression of your design intent, it can also connect transparently to different \"backends\". This not only incldues numpy, but also different deep learning frameworks, which map the program to different hardware, some can be as large as an entire cluster in a data center with tens of thought GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e6653-ac08-4100-96a7-2b8a371010a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
